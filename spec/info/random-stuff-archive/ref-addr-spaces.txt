Misc ai prompts & thoughts:

The point of addr spaces, is when you physically store/read/write from it in a different way. (32bit addr, 64bit addr, gpu addresses, ...)


Hi, im making a prog lang, and i ran into a problem.
Currently references are just aliased+immutable or unique+mutable, and a lifetime.
But with gpu support, i need address space support too.
Luckily references cant be turned into addresses, so that simplifies things, but it doesnt help with the fact that some references can be read by the cpu ("special register" reference, memory reference) and some can only be read by the gpu.
How should i solve this? how do existing languages solve this?




I would like to, by default, allow code to run on both the gpu & the cpu, so this should work on both too:
`fn computeFft(src = *[]f32, dst = *mut[]f32) --Note: * is the ref type symbol, & is the create-reference-to operator`

And only through some library call thingy would the gpu be used.
`gpulib.run(computeFft, { &stackSlice, &stackOut })` -> Somehow an error is created, due to using non-gpu memory.
`gpulib.run(computeFft, { gpuMemInput, gpuMemOut })` -> Works fine

I would also like for functions to be able to explicitly opt-in to references that they arent able to read/write.

=> ref types by default could be any addr-space that can be R/W
=> ref types need to store what addr space they are (after monomorphistation atleast)
=> we prob want to support custom addr spaces for special architectures (maybe Scoreboard/NbtStorage in minecraft for example)

Might potentially need a way to have generic addr-space stuff:
`fn apple(a = *in(space)[]f32, b = *in(space) mut[]f32, const space = Space)`
`fn apple(apple = *[]f32, b = *in apple mut[]f32)` -> Requires `b` to have the same addr-space as `apple`



By default refs require the addr-space to support r/w (w only for mutable refs) (atleast for type coercions i mean)
=> Works fine for dev-local stuff.

`gpulib.run` under the hood could obtain a GPU-fn from `computeFft` (at compile time ofc, so thats a const arg)
=> but how would it type check that fn-call?
(somehow checks it), so how does the type check fail/pass?
=> In the invalid example you pass `*in Cpu` into a gpu-fn that wants a (gpu)`*` compatible type, so
    either it knows its `*in Cpu` and stops it, or it doesnt know its `*in Gpu` and isnt sure, so stops it.
=> In this case it knows nothing. So it has no proof its on the gpu, and thus cant be sure if it will work. (gpulib cant actually run on the gpu so it technically will never be on the gpu anyway)
=> In the good one, you pass `*in Gpu` into the gpu-fn, and its fine (it knows its a `*in Gpu`, cuz otherwise you could r/w it from the cpu).

What about single dev, incompatible addr spaces?
=> Add a const-arg and use that as the addr space for ur ref.
=> The ref is "useless", but can be returned or copied in some cases.

*#(Global) /my.lifetime/lifetime2 mut T
*%(Global) /my.lifetime/lifetime2 mut T
*$(Global) /my.lifetime/lifetime2 mut T
*<(Global)< /my.lifetime/lifetime2 mut T
*>(Global)> /my.lifetime/lifetime2 mut T
*>(Global)< /my.lifetime/lifetime2 mut T
*^(Global) /my.lifetime/lifetime2 mut T
*<(Global)> /my.lifetime/lifetime2 mut T
*:(Global) /my.lifetime/lifetime2 mut T
*/my.lifetime/lifetime2 :(Global) mut T
*'(Global) /my.lifetime/lifetime2 mut T
*/my.lifetime/lifetime2 '(Global) mut T
*in(Global) /my.lifetime/lifetime2 mut T

: => anoying to parse. (due to :>)
' => reminds of rust lifetimes. But is that an issue? Conflicts with loop/do-expr labels.
^ => its pointing up, and away from the type, a bit odd.
%,$,# => too noisy
>>, <<, ><, <>. => anoying to type sometimes, due to 2 chars.


@MyAttr{} ex *:Heap /a/b mut std::Box(T) @<OuterAnnotation{}
@MyAttr{} ex */a/b :Heap mut std::Box(T) @<OuterAnnotation{}
@MyAttr{} ex *'Heap /a/b mut std::Box(T) @<OuterAnnotation{}
@MyAttr{} ex */a/b 'Heap mut std::Box(T) @<OuterAnnotation{}
@MyAttr{} ex *@Heap /a/b mut std::Box(T) @<OuterAnnotation{}
@MyAttr{} ex *in Heap /a/b mut std::Box(T) @<OuterAnnotation{}


Additional info:

So, i dont want to monomorphize (at the analysis level) based on addr-space (it could multiply func count by more than 2!!!), unless its due to it being in a const arg.

To help solve the unknown questions, can you write a implementation for `gpulib.run`
Its args would be `const func = ReflectedFunction, args=func.argsT()`


Ai resp:

1. The Opcode (The Instruction Itself) 

This is the most common case. The value of the pointer might look the same (e.g., 0x1000), but the CPU/GPU instruction to access it is different. 

    GPU Example (NVIDIA PTX): 
         If Space is Global (VRAM): The compiler emits ld.global.f32 (Slow, hundreds of cycles latency).
         If Space is Shared (On-chip): The compiler emits ld.shared.f32 (Extremely fast, ~30 cycles).
         Result: If you try to use a Global Address with a Shared instruction, the GPU hardware will crash or read garbage.
?! also: ld.const.f32 !
     
         2. The Address Width & Format 

You mentioned 32-bit vs 64-bit. In some architectures, different spaces have different pointer sizes, even on the same machine. 
    Harvard Architecture (Microcontrollers/AVR): 
         Code Space: Addresses might be 24-bit (to address large flash memory).
         Data Space: Addresses might be 16-bit (to address smaller RAM).
         Physical Difference: The electrical wires (buses) for code and data are physically separate.

    Segmented Addressing (x86 Real Mode / Embedded): 
         Near Address: 16-bit offset (Fast).
         Far Address: 16-bit segment + 16-bit offset (Slow, requires changing segment registers).
         In your language: Addr<u16, Space::Near> vs Addr<u32, Space::Far>.


 3. The Cache Coherency Protocol 
This is invisible in the instruction set but critical for correctness. "Writing in a different way" often means "How do we tell other cores that we changed data?" 

    Write-Back (Standard RAM): 
         The CPU writes to L1/L2 cache. It doesn't touch main RAM immediately.
         Physical Cost: Fast write, but requires a "Cache Flush" or "Memory Fence" if another core/DMA needs to see it.
        
    Write-Through / Uncached (MMIO / Framebuffers): 
         The CPU writes directly to the memory controller, bypassing cache.
         Physical Cost: Very slow write, but guaranteed to be visible to the hardware immediately.
         
    GPU Local/Shared: 
         Often has no cache coherency at all. If Thread A writes to Shared Memory, Thread B will not see it until a specific barrier.sync instruction is executed.
         Your Space::Shared type enforces that you cannot create a reference without passing a sync barrier


Summary of your Spaces 
So, when you define your enum, you are defining the Physical Bus logic: 

Space Name      Physical Instruction/Bus Behavior
 
Generic     	Standard cached Load/Store (CPU MOV, GPU LD.G) 
Shared      	On-chip scratchpad Load/Store (GPU LD.S) 
IO      	    Uncached/Volatile Load/Store (Prevents optimization) 
Thread      	Stack frame access (Base Pointer + Offset)


=> hmm not sure about the io or thread...

Why keep Thread? 

Even though it's "just a stack," physically it is often accessed differently. 

     On CPUs: Global addresses are absolute (0x0050_A123). Thread/Stack addresses are relative (SP + 0x10). This makes the code smaller and position-independent.
     On GPUs: There is a dedicated "Private" memory space. If you don't have a Thread space type, you can't safely distinguish between "Thread local variable" and "Pointer to global memory that happens to only be used by one thread."
     

Heap (The standard global RAM)
     Physical: Standard cached load/store. Read-Write.
     Usage: Standard variables, structs, buffers.
      
Const (Read-Only global memory)
     Physical: Maps to .rodata, Flash memory, or GPU Texture Units. These often have separate, higher-bandwidth caches dedicated solely to reading.
     Usage: Lookup tables, shaders, string literals, configuration data.
     Note: Trying to write to this space triggers a hardware fault/exception.
      
Thread (Private/Stack memory)
     Physical: Uses relative addressing (Stack Pointer + Offset) or dedicated "Private" memory banks on GPUs. Extremely high bandwidth, no sharing.
     Usage: Function stacks, loop accumulators, GPU lane-local variables.
      
Shared (Group shared memory)
     Physical: On-chip scratchpad (L1 / SRAM). Managed software-defined cache. Requires barriers (sync) for visibility.
     Usage: Communication between parallel threads (CUDA Shared / LDS).
          